# 47 NIO 服务器（三）之 Telnet 层 #
dubbo-remoting-api 模块， telnet 包，Telnet 命令。

Dubbo 支持通过 telnet 命令，用来服务治理。其中，clear exit help log status 通用指令，通过 telnet 包实现。而其它几个指令，需要不同协议( Protocol )自己实现。目前，仅有 Dubbo Protocol 实现了自定义指令。

主要分成三种不通的类。

- TelnetCodec ：负责编解码 Telnet 命令与结果。
- TelnetHandlerAdapter ：负责接收来自 HeaderExchangeHandler 的 telnet 命令，分发给对应的 TelnetHandler 实现类，进行处理，返回命令结果。
- XXXTelnetHandler ：处理对应的 telnet 命令，返回结果。

流程如下图：

![](/picture/dubbo-telnet-flow.png)

## 47.1 TelnetCodec ##

com.alibaba.dubbo.remoting.telnet.codec.TelnetCodec ，实现 TransportCodec 类，Telnet 命令编解码器。

**解码**

//  todo dubbo运维了解

----

# 48 NIO 服务器（四）之 Exchange 层 #
dubbo-remoting-api 模块， exchange 包，信息交换层。

exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer。

在一次 RPC 调用，每个请求( Request )，是关注对应的响应( Response )。那么 transport 层 提供的网络传输 功能，是无法满足 RPC 的诉求的。因此，exchange 层，在其 Message 之上，构造了Request-Response 的模型。

实现上，也非常简单，将 Message 分成 Request 和 Response 两种类型，并增加编号属性，将 Request 和 Response 能够一一映射。

实际上，RPC 调用，会有更多特性的需求：1）异步处理返回结果；2）内置事件；3）等等。因此，Request 和 Response 上会有类似编号的系统字段。

一条消息，我们分成两段：
- 协议头( Header ) ： 系统字段，例如编号等。
- 内容( Body ) ：具体请求的参数和响应的结果等。

dubbo心跳检测：
dubbo心跳时间heartbeat默认是60s，超过heartbeat时间没有收到消息，就发送心跳消息(provider，consumer一样),如果连着3次(heartbeatTimeout为heartbeat*3)没有收到心跳响应，provider会关闭channel，而consumer会进行重连;不论是provider还是consumer的心跳检测都是通过启动定时任务的方式实现；

## 48.1 ExchangeChannel ##

com.alibaba.dubbo.remoting.exchange.ExchangeChannel ，继承 Channel 接口，信息交换通道接口

### 48.1.1 HeaderExchangeChannel ###

com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeChannel ，实现 ExchangeChannel 接口，基于消息头部( Header )的信息交换通道实现类。

#### 48.1.1.1 构造方法 ####

````
private static final String CHANNEL_KEY = HeaderExchangeChannel.class.getName() + ".CHANNEL";

/** 通道。HeaderExchangeChannel 是传入 channel 属性的装饰器，每个实现的方法，都会调用 channel 。 */
private final Channel channel;
/** 是否关闭 */
private volatile boolean closed = false;

HeaderExchangeChannel(Channel channel) {
    if (channel == null) {
        throw new IllegalArgumentException("channel == null");
    }
    this.channel = channel;
}
````

getOrAddChannel(Channel) 静态方法，创建 HeaderExchangeChannel 对象。

基本流程如下：
1. 传入的 ch 属性，实际就是 HeaderExchangeChanel.channel 属性。
2. 通过 ch.attribute 的 CHANNEL_KEY 键值，保证有且仅有为 ch 属性，创建唯一的 HeaderExchangeChannel 对象。
3. 要求已连接。

removeChannelIfDisconnected(ch) 静态方法，移除 HeaderExchangeChannel 对象。

#### 48.1.1.2 发送请求 ####

基本流程如下：

1. 若已经关闭，不再允许发起新的请求。
2. 第 6 至 10 行：创建 Request 对象。其中，twoWay = true 需要响应；data = request 具体数据。
3. 创建 DefaultFuture 对象。
4. 调用 Channel#send(req) 方法，发送请求。
5. 发生 RemotingException 异常，调用 DefaultFuture#cancel() 方法，取消。
6. 返回 DefaultFuture 对象。从代码的形式上来说，有点类似线程池提交任务，返回 Future 对象。

#### 48.1.1.3 优雅关闭 ####

基本流程如下：
1. 标记 closed = true ，避免发起新的请求。
2. 调用 DefaultFuture#hasFuture(channel) 方法，判断已发起的已经是否已经都响应了。若否，等待完成或超时。
3. 关闭通道。

## 48.2 ExchangeClient ##
com.alibaba.dubbo.remoting.exchange.ExchangeClient ，实现 Client ，ExchangeChannel 接口，信息交换客户端接口。

### 48.2.1 HeaderExchangeClient ###
com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeClient ，实现 ExchangeClient 接口，基于消息头部( Header )的信息交换客户端实现类。

**构造方法**

````
/** 定时器线程池 */
private static final ScheduledThreadPoolExecutor scheduled = new ScheduledThreadPoolExecutor(2, new NamedThreadFactory("dubbo-remoting-client-heartbeat", true));
/** 客户端 */
private final Client client;
/** 信息交换通道 */
private final ExchangeChannel channel;
// heartbeat timer
/** 心跳定时器 */
private ScheduledFuture<?> heartbeatTimer;
/** 是否心跳 */
private int heartbeat;
// heartbeat timeout (ms), default value is 0 , won't execute a heartbeat.
/** 心跳间隔，单位：毫秒 */
private int heartbeatTimeout;
public HeaderExchangeClient(Client client, boolean needHeartbeat) {
    if (client == null) {
        throw new IllegalArgumentException("client == null");
    }
    this.client = client;
    // 创建 HeaderExchangeChannel 对象
    this.channel = new HeaderExchangeChannel(client);
    // 读取心跳相关配置
    String dubbo = client.getUrl().getParameter(Constants.DUBBO_VERSION_KEY);
    this.heartbeat = client.getUrl().getParameter(Constants.HEARTBEAT_KEY, dubbo != null && dubbo.startsWith("1.0.") ? Constants.DEFAULT_HEARTBEAT : 0);
    this.heartbeatTimeout = client.getUrl().getParameter(Constants.HEARTBEAT_TIMEOUT_KEY, heartbeat * 3);
    if (heartbeatTimeout < heartbeat * 2) { // 避免间隔太短
        throw new IllegalStateException("heartbeatTimeout < heartbeatInterval * 2");
    }
    // 发起心跳定时器
    if (needHeartbeat) {
        startHeatbeatTimer();
    }
}
````

基本流程如下;
1. 使用传入的 client 属性，创建 HeaderExchangeChannel 对象。
2. 读取心跳相关配置。默认，开启心跳功能。
	1. 心跳间隔，对于长连接，当物理层断开时，比如拔网线，TCP的FIN消息来不及发送，对方收不到断开事件，此时需要心跳来帮助检查连接是否已断开
3.  调用 #startHeatbeatTimer() 方法，发起心跳定时器。

## 48.3 ExchangeServer ##

com.alibaba.dubbo.remoting.exchange.ExchangeServer ，继承 Server 接口，信息交换服务器接口

### 48.3.1 HeaderExchangeServer ###
com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeServer ，实现 ExchangeServer 接口，基于消息头部( Header )的信息交换服务器实现类。

- 代码实现上，和 HeaderExchangeChannel + HeaderExchangeClient 的综合。

#### 48.3.1.1 构造方法 ####
代码实现上，和 HeaderExchangeClient 的类似。

````
/** 定时器线程池 */
private final ScheduledExecutorService scheduled = Executors.newScheduledThreadPool(1, new NamedThreadFactory("dubbo-remoting-server-heartbeat", true));
/** 服务器 */
private final Server server;
// heartbeat timer
/** 心跳定时器 */
private ScheduledFuture<?> heatbeatTimer;
/** 是否心跳 */
// heartbeat timeout (ms), default value is 0 , won't execute a heartbeat.
private int heartbeat;
/** 心跳间隔，单位：毫秒 */
private int heartbeatTimeout;
/** 是否关闭 */
private AtomicBoolean closed = new AtomicBoolean(false);

public HeaderExchangeServer(Server server) {
    if (server == null) {
        throw new IllegalArgumentException("server == null");
    }
    // 读取心跳相关配置
    this.server = server;
    this.heartbeat = server.getUrl().getParameter(Constants.HEARTBEAT_KEY, 0);
    this.heartbeatTimeout = server.getUrl().getParameter(Constants.HEARTBEAT_TIMEOUT_KEY, heartbeat * 3);
    if (heartbeatTimeout < heartbeat * 2) {
        throw new IllegalStateException("heartbeatTimeout < heartbeatInterval * 2");
    }
    // 发起心跳定时器
    startHeatbeatTimer();
}
````

#### 48.3.1.2 优雅关闭 ####
Server 关闭的过程，分成两个阶段：正在关闭和已经关闭。

基本流程如下：
1. 调用 #startClose() 方法，标记正在关闭。
2. 发送 READONLY 事件给所有 Client ，表示 Server 不再接收新的消息，避免不断有新的消息接收到。
	1. 即使 client 处于连接中，但是 Server 处于正在关闭中，也算不可用，不进行发送请求( 消息 )。
3. sendChannelReadOnlyEvent() 方法，广播客户端，READONLY_EVENT 事件
4. 调用 #oClose() 方法，关闭心跳定时器。
5. 真正关闭服务器。

### 48.3.2 ExchangeServerDelegate ###
com.alibaba.dubbo.remoting.exchange.support.ExchangeServerDelegate ，实现 ExchangeServer 接口，信息交换服务器装饰者。在每个实现的方法里，直接调用被装饰的 server 属性的方法。

## 48.4 请求/响应模型 ##
### 48.4.1 Request ###
com.alibaba.dubbo.remoting.exchange.Request ，请求

内置两种事件：

- HEARTBEAT_EVENT ：心跳。因为心跳比较常用，所以在事件上时候了 null 。
- READONLY_EVENT ：只读。

属性：
- mId 属性：编号。使用 INVOKE_ID 属性生成，JVM 进程内唯一。
- mTwoWay 属性，标记请求是否响应( Response )，默认需要。
- mBroken 属性，是否异常的请求。在消息解析的时候，会出现。
- mData 属性，请求具体数据。

### 48.4.2 Response ###
com.alibaba.dubbo.remoting.exchange.Response ，响应。

- mId 属性，响应编号，和请求编号一致。
- mStatus 属性，状态。有多种状态：[状态码] (dubbo-remoting/dubbo-remoting-api/src/main/java/com/alibaba/dubbo/remoting/exchange/Response.java)。
- mEvent 属性，是否事件。和 Request 内置了一样的事件，但是 READONLY_EVENT 并未使用。因为目前，只读事件，无需响应。
- mErrorMsg 属性，错误消息。
- mResult 属性，结果。

### 48.4.3 ResponseFuture ###
com.alibaba.dubbo.remoting.exchange.ResponseFuture ，响应 Future 接口。
和 java.util.concurrent.Future 很类似。

#### 48.4.3.1 DefaultFuture ####
com.alibaba.dubbo.remoting.exchange.support.DefaultFuture ，实现 ResponseFuture 接口，默认响应 Future 实现类。同时，它也是所有 DefaultFuture 的管理容器。

**构造方法**

````
/**
 * 通道集合
 * key：请求编号
 */
private static final Map<Long, Channel> CHANNELS = new ConcurrentHashMap<Long, Channel>();
/**
 * Future 集合
 * key：请求编号
 */
private static final Map<Long, DefaultFuture> FUTURES = new ConcurrentHashMap<Long, DefaultFuture>();

/** 请求编号 */
// invoke id.
private final long id;
/** 通道 */
private final Channel channel;
/** 请求 */
private final Request request;
/** 超时 */
private final int timeout;
/** 创建开始时间 */
private final long start = System.currentTimeMillis();
/** 发送请求时间 */
private volatile long sent;
/** 响应 */
private volatile Response response;
/** 回调 */
private volatile ResponseCallback callback;

public DefaultFuture(Channel channel, Request request, int timeout) {
    this.channel = channel;
    this.request = request;
    this.id = request.getId();
    this.timeout = timeout > 0 ? timeout : channel.getUrl().getPositiveParameter(Constants.TIMEOUT_KEY, Constants.DEFAULT_TIMEOUT);
    // put into waiting map.
    FUTURES.put(id, this);
    CHANNELS.put(id, channel);
}
````

CHANNELS 静态属性，通道集合。通过 #hasFuture(channel) 方法，判断通道是否有未结束的请求。
FUTURES 静态属性，Future 集合。
sent 属性，发送请求时间。因为在目前 Netty Mina 等通信框架中，发送请求一般是异步的，因此在 ChannelHandler#sent(channel, message) 方法中，调用 DefaultFuture#sent(channel, request) 静态方法，
callback 属性，回调，适用于异步请求。通过 #setCallback(callback) 方法设置。

**获得值**
get(timeout)函数：
基本流程如下：
1. 调用 #isDone() 方法，判断是否完成。若未完成，基于 Lock + Condition 的方式，实现等待。而等待的唤醒，通过 ChannelHandler#received(channel, message) 方法，接收到请求时执行 DefaultFuture#received(channel, response) 方法。
	1. 获得开始时间。注意，此处使用的不是 start 属性。后面我们会看到，#get(...) 方法中，使用的是重新获取开始时间；后台扫描调用超时任务，使用的是 start 属性。也就是说，#get(timeout) 方法的 timeout 参数，指的是从当前时刻开始的等待超时时间。当然，这不影响最终的结果，最终 Response 是什么，由是 ChannelHandler#received(channel, message) 还是后台扫描调用超时任务，谁先调用 DefaultFuture#received(channel, response) 方法决定。
	2. 获得锁。
	3. 等待完成或超时。
	4. 释放锁。
	5. 若未完成，抛出超时异常 TimeoutException 。
		1. TimeoutException.phase 的阶段，由 sent > 0 来决定，即 Client 是否发送给 Server 。
		2. getTimeoutMessage(scan) 方法，获得超时异常提示信息。
2. 调用 #returnFromResponse() 方法，返回响应( Response )

**响应结果**

````
public static void received(Channel channel, Response response) {
    try {
        // 移除 FUTURES
        DefaultFuture future = FUTURES.remove(response.getId());
        // 接收结果
        if (future != null) {
            future.doReceived(response);
        } else {
            logger.warn("The timeout response finally returned at " + (new SimpleDateFormat("yyyy-MM-dd HH:mm:ss.SSS").format(new Date())) + ", response " + response + (channel == null ? "" : ", channel: " + channel.getLocalAddress() + " -> " + channel.getRemoteAddress()));
        }
    // 移除 CHANNELS
    } finally {
        CHANNELS.remove(response.getId());
    }
}
````

基本流程如下：
1. 移除 FUTURES 
2. 调用 DefaultFuture#doReceived(response) 方法，响应结果。
	1. 获得锁。
	2. 设置响应 response 。
	3. 调用 Condition#signal() 方法，通知，唤醒 DefaultFuture#get(..) 方法的等待。
	4. 释放锁。
	5. 调用 #invokeCallback(callback) 方法，执行回调方法。
3. 超时情况，打印告警日志。
4. 移除 CHANNELS 。

**设置回调**
基本流程如下：
1. 若已完成，调用 #invokeCallback(callback) 方法，执行回调方法。
2. 获得锁。
3. 若未完成，设置回调 callback 属性，等在 #doReceived(response) 方法中再回调。
4. 标记已完成。调用 #invokeCallback(callback) 方法，执行回调方法。
5. 释放锁。

**调用回调**
和 #returnFromResponse() 方法，情况一致。

基本流程如下：
1. 正常返回，调用 ResponseCallback#done(result) 方法，处理结果。
2. 超时异常，调用 ResponseCallback#caught(e) 方法，处理 TimeoutException 异常。
3. 其他异常，调用 ResponseCallback#caught(e)` 方法，处理 RuntimeException 异常。

**后台扫描调用超时任务**
基本流程如下：
1. 已完成，跳过
2. 超时
3. 创建超时 Response
4. 响应结果

#### 48.4.3.2 SimpleFuture**** ####
com.alibaba.dubbo.remoting.exchange.support.SimpleFuture ，实现 ResponseFuture 接口，简单的 Future 实现。

### 48.4.4 MultiMessage ###
com.alibaba.dubbo.remoting.exchange.support.MultiMessage ，实现 Iterable 接口，多消息的封装。

## 48.5 Handler ##
### 48.5.1 HeartbeatHandler ###
com.alibaba.dubbo.remoting.exchange.support.header.HeartbeatHandler ，实现 AbstractChannelHandlerDelegate 抽象类，心跳处理器，处理心跳事件。

#### 48.5.1.1 HeartBeatTask ####
com.alibaba.dubbo.remoting.exchange.support.header.HeartBeatTask ，实现 Runnable 接口，心跳任务。

channelProvider 属性，用于查询获得需要心跳的通道数组。

**执行任务**
基本流程如下：

1. 【任务一】最后读或写的时间，任一超过心跳间隔 heartbeat ，发送心跳。
2. 【任务二】最后读的时间，超过心跳超时时间 heartbeatTimeout ，分成两种情况：
3. 客户端侧，重连连接服务端。
4. 服务端侧，关闭客户端连接。

### 48.5.2 HeaderExchangeHandler ###
com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeHandler，实现 ChannelHandlerDelegate 接口，基于消息头部( Header )的信息交换处理器实现类。

**接收消息**
基本流程如下：
1. 设置最后的读时间。
2. 创建 ExchangeChannel 对象。
3. 处理请求( Request)
	1. 调用 #handlerEvent(channel, request) 方法，处理事件请求。
	2. 调用 #handleRequest(channel, request) 方法，处理普通请求（需要响应），并将响应写回请求方。
	3. 调用 ChannelHandler#received(channel, message) 方法，处理普通请求（无需响应）。
4. 调用 #handleResponse(channel, message) 方法，处理响应。
5. 处理 String 的情况
	1. 客户端侧，不支持 String 的情况。
	2. 服务端侧，目前仅有 telnet 命令的情况，调用 TelnetHandler#telnet(channel, message) 方法，获得 telnet 命令的结果，并响应给 telnet 客户端。
6. 剩余的情况，调用 ChannelHandler#received(channel, message) 方法，处理。
7. 移除 ExchangeChannel 对象，若已断开。

**发生异常**
当发生 ExecutionException 异常，返回异常响应( Response )。目前会发生 ExecutionException 的情况，并且符合提交

### 48.5.3 ExchangeHandler ###
com.alibaba.dubbo.remoting.exchange.ExchangeHandler ，继承 ChannelHandler 和 TelnetHandler 接口，信息交换处理器接口。
- 注意，返回的是请求结果。正如我们在上文看到的，将请求结果，设置到 Response.mResult 属性中。

#### 48.5.3.1 ExchangeHandlerAdapter ####

com.alibaba.dubbo.remoting.exchange.support.ExchangeHandlerAdapter ，实现 ExchangeHandler 接口，继承 TelnetHandlerAdapter 抽象类，信息交换处理器适配器抽象类。

在 DubboProtocol 、ThirftProtocol 中，都会基于 ExchangeHandlerAdapter 实现自己的处理器，处理请求，返回结果。

### 48.5.4 Replier ###
//  todo 目前仅用于p2p

## 48.6 Exchanger ##
com.alibaba.dubbo.remoting.exchange.Exchanger ，数据交换者接口。
Exchanger 和 Transporter 类似。

- SPI(HeaderExchanger.NAME) 注解，Dubbo SPI 拓展点，默认为 "header"，即 HeaderExchanger 。
- Adaptive({Constants.EXCHANGER_KEY}) 注解，基于 Dubbo SPI Adaptive 机制，加载对应的 Server 实现，使用 URL.exchanger 属性。
- Adaptive({Constants.EXCHANGER_KEY}) 注解，基于 Dubbo SPI Adaptive 机制，加载对应的 Client 实现，使用 URL.exchanger 属性。

### 48.6.1 HeaderExchanger ###
com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchanger ，实现 Exchanger 接口，基于消息头部( Header )的信息交换者实现类。

### 48.6.2 Exchangers ###
Exchangers 和 Transporters 类似。
com.alibaba.dubbo.remoting.Transporters ，数据交换者门面类

## 48.7 ExchangeCodec ##
com.alibaba.dubbo.remoting.exchange.codec.ExchangeCodec ，继承 TelnetCodec 类，信息交换编解码器。

![](/picture/dubbo-nio-exchanger-codec.png)


- 基于消息长度的方式，做每条消息的粘包拆包处理。
- Header 部分，协议头，通过 Codec 编解码。Bits 位如下：
	- [0, 15]：Magic Number
	- [16, 20]：Serialization 编号。
	- [21]：event 是否为事件。
	- [22]：twoWay 是否需要响应。
	- [23]：是请求还是响应。
	- [24 - 31]：status 状态。
	- [32 - 95]：id 编号，Long 型。
	- [96 - 127]：Body 的长度。通过该长度，读取 Body 。
- Body 部分，协议体，通过 Serialization 序列化/反序列化。

**属性**

````
// header length.
protected static final int HEADER_LENGTH = 16;
// magic header.
protected static final short MAGIC = (short) 0xdabb;
protected static final byte MAGIC_HIGH = Bytes.short2bytes(MAGIC)[0];
protected static final byte MAGIC_LOW = Bytes.short2bytes(MAGIC)[1];
// message flag.
protected static final byte FLAG_REQUEST = (byte) 0x80; // 128
protected static final byte FLAG_TWOWAY = (byte) 0x40; // 64
protected static final byte FLAG_EVENT = (byte) 0x20; // 32
protected static final int SERIALIZATION_MASK = 0x1f; // 31
````
HEADER_LENGTH 静态属性，Header 总长度，16 Bytes = 128 Bits 。

**编码**
基本流程如下：
1. 调用 #encodeRequest(channel, buffer, request) 方法，编码请求。
2. 调用 #encodeResponse(channel, buffer, response) 方法，编码响应。
3. 调用 TelnetCodec#encode(channel, buffer, msg) 方法，编码 Telnet 命令的结果。

encodeRequest(channel, buffer, request) 方法基本流程：
1. Header 部分，先写入 header 数组，再写入 Buffer 中。
2. Body 部分，使用 Serialization 序列化 Request.data ，写入到 Buffer 中。
3. 调用 #checkPayload(channel, len) 方法，校验 Body 内容的长度

- 为什么 Buffer 先写入了 Body ，再写入 Header 呢？因为 Header 中，里面 [96 - 127] 的 Body 长度，需要序列化后才得到。

**解码**

基本流程如下：
1. 读取 header 数组。注意，这里的 Math.min(readable, HEADER_LENGTH) ，优先考虑解析 Dubbo 协议。
2. 调用 #decode(channel, buffer, readable, header) 方法，解码。
3. 基于消息长度的方式，拆包。
4. 调用 #decodeBody(channel, is, header) 方法，解析 Header + Body ，根据情况，返回 Request 或 Reponse 。🙂 逻辑上，是 #encodeRequest(...) 和 #encodeResponse(...) 方法的反向，
5. skip 未读完的流

----

# 49 NIO 服务器（五）之 Buffer 层 #

dubbo-remoting-api 模块， buffer 包，Buffer 层。
Buffer 在 NIO 框架中，扮演非常重要的角色，基本每个库都提供了自己的 Buffer 实现，例如：
- Java NIO 的 java.nio.ByteBuffer
- Mina 的 org.apache.mina.core.buffer.IoBuffer
- Netty4 的 io.netty.buffer.ByteBuf

在 dubbo-remoting-api 的 buffer 包中，一方面定义了 ChannelBuffer 和 ChannelBufferFactory 的接口，同时提供了多种默认的实现。

## 49.1 ChannelBuffer ##
com.alibaba.dubbo.remoting.buffer.ChannelBuffer ，实现 Comparable 接口，通道 Buffer 接口。

ChannelBuffer 在接口方法的定义上，主要参考了 Netty 的 ByteBuf 进行设计，所以接口和注释基本一致

独有的接口方法 #factory() 方法，用于逻辑中，需要创建 ChannelBuffer 的情况。

### 49.1.1 AbstractChannelBuffer ###
com.alibaba.dubbo.remoting.buffer.AbstractChannelBuffer ，实现 ChannelBuffer 接口，通道 Buffer 抽象类。

**构造方法**
````
/** 读取位置 */
private int readerIndex;
/** 写入位置 */
private int writerIndex;
/** 标记的读取位置 */
private int markedReaderIndex;
/** 标记的写入位置 */
private int markedWriterIndex;
````

**实现方法**
在 AbstractChannelBuffer 实现的方法，都是重载的方法，真正实质的方法，需要子类来实现。


### 49.1.2 ByteBufferBackedChannelBuffer ###
com.alibaba.dubbo.remoting.buffer.ByteBufferBackedChannelBuffer ，实现 AbstractChannelBuffer 抽象类，基于 java.nio.ByteBuffer 的 Buffer 实现类。

**构造方法**
````
/**
 * buffer
 * java.nio.ByteBuffer
 */
private final ByteBuffer buffer;
/** 容量 */
private final int capacity;

public ByteBufferBackedChannelBuffer(ByteBuffer buffer) {
    if (buffer == null) {
        throw new NullPointerException("buffer");
    }
    // buffer
    this.buffer = buffer.slice();
    // 容量
    capacity = buffer.remaining();
    // 设置 `writerIndex`
    writerIndex(capacity);
}

public ByteBufferBackedChannelBuffer(ByteBufferBackedChannelBuffer buffer) {
    // buffer
    this.buffer = buffer.buffer;
    // 容量
    capacity = buffer.capacity;
    // 设置 `writerIndex` `readerIndex`
    setIndex(buffer.readerIndex(), buffer.writerIndex());
}
````
**工厂**
对应的工厂是 DirectChannelBufferFactory 或 HeapChannelBufferFactory 。

### 49.1.3 HeapChannelBuffer ###
com.alibaba.dubbo.remoting.buffer.HeapChannelBuffer ，实现 AbstractChannelBuffer 抽象类，基于字节数组的 Buffer 实现类。

**工厂**
对应的工厂是 HeapChannelBufferFactory 。

### 49.1.4 DynamicChannelBuffer ###
com.alibaba.dubbo.remoting.buffer.DynamicChannelBuffer ，实现 AbstractChannelBuffer 抽象类，基于动态的 Buffer 实现类。或者说，基于传入的 ChannelBufferFactory 的 Buffer 实现类。

## 49.2 ChannelBuffers ##
com.alibaba.dubbo.remoting.buffer.ChannelBuffers ，Buffer 工具类，提供创建、比较 ChannelBuffer 等公用方法。

## 49.3 ChannelBufferFactory ##
com.alibaba.dubbo.remoting.buffer.ChannelBufferFactory ，通道 Buffer 工厂接口。

### 49.3.1 DirectChannelBufferFactory ###
com.alibaba.dubbo.remoting.buffer.DirectChannelBufferFactory ，实现 ChannelBufferFactory 接口，创建 DirectChannelBuffer 的工厂。

### HeapChannelBufferFactory ###
com.alibaba.dubbo.remoting.buffer.HeapChannelBufferFactory ，实现 ChannelBufferFactory 接口，创建 HeapChannelBufferFactory 的工厂。

## 49.4 IO ##
实际 IO 操作，是基于 InputStream 和 OutputStream 

### 49.4.1 ChannelBufferInputStream ###
com.alibaba.dubbo.remoting.buffer.ChannelBufferInputStream ，实现 InputStream 接口

### 49.4.2 ChannelBufferOutputStream ###
com.alibaba.dubbo.remoting.buffer.ChannelBufferOutputStream ，实现 OutputStream 接口

----

# 50 NIO 服务器（六）之 Netty4 实现 #
NIO服务器netty4抽象实现。

## 50.1 NettyTransporter ##
com.alibaba.dubbo.remoting.transport.netty4.NettyTransporter ，实现 Transporter 接口，基于 Netty4 的网络传输实现类。

````
public class NettyTransporter implements Transporter {
    /** 拓展名 */
    public static final String NAME = "netty4";
    public Server bind(URL url, ChannelHandler listener) throws RemotingException {
        return new NettyServer(url, listener);
    }
    public Client connect(URL url, ChannelHandler listener) throws RemotingException {
        return new NettyClient(url, listener);
    }
}
````

- NAME 静态属性，拓展名。
- NettyTransporter 基于 Dubbo SPI 机制加载。
- 创建 NettyServer 和 NettyClient 对象。

## 50.2 NettyChannel ##
io.netty.channel.ChannelFuture.NettyChannel ，实现 AbstractChannel 抽象类，封装 Netty Channel 的通道实现类。

````
/**
 * 通道集合
 * channelMap 静态属性，通道集合。在实际 Netty Handler 里（例如下面我们会看到的 NettyServerHandler 和 NettyClientHandler），每个方法参数里，传递的是 io.netty.channel.Channel 对象。通过 NettyChannel.channelMap 中，获得对应的 NettyChannel 对象。
 */
private static final ConcurrentMap<io.netty.channel.Channel, NettyChannel> channelMap = new ConcurrentHashMap<Channel, NettyChannel>();

/**
 * 通道
 * channel 属性，通道。NettyChannel 是传入 channel 属性的装饰器，每个实现的方法，都会调用 channel 。
 */
private final io.netty.channel.Channel channel;
/**
 * 属性集合
 * attributes 属性，属性集合。注意，setAttribute(...) 等方法，使用的是该属性，而不是 io.netty.channel.Channel 的。
 */
private final Map<String, Object> attributes = new ConcurrentHashMap<String, Object>();

private NettyChannel(io.netty.channel.Channel channel, URL url, ChannelHandler handler) {
    super(url, handler);
    if (channel == null) {
        throw new IllegalArgumentException("netty channel == null;");
    }
    this.channel = channel;
}
````
内部函数：
getOrAddChannel(ch, url, handler) 静态方法，创建 NettyChannel 对象。
removeChannelIfDisconnected(ch) 静态方法，移除 NettyChannel 对象。

**发送消息**
send(message, sent)

基本流程如下：
1. 调用 #send(message, sent) 方法，检查连接状态。
2. success ，是否执行成功。若不需要等待发送成功( sent = false ) ，默认成功。
3. 调用真正的 io.netty.channel.Channel#writeAndFlush(message) 方法，发送消息。
4. 若需要等待发送成功( sent = true )，等待直到成功或超时。
5. 若发生异常，抛出异常。
6. 若发送失败，抛出异常。

**关闭通道**
close()

## 50.3 Server ##
### 50.3.1 NettyServer ###
com.alibaba.dubbo.remoting.transport.netty4.NettyServer ，实现 Server 接口，继承 AbstractServer 抽象类，Netty 服务器实现类。

包括启动server和关闭server的方法。

### 50.3.2 NettyServerHandler ###
com.alibaba.dubbo.remoting.transport.netty4.NettyServerHandler ，实现 io.netty.channel.ChannelDuplexHandler 类，NettyServer 的处理器。

获取context并把congtext交给handler处理。

## 50.4 Client ##
### 50.4.1 NettyClient ###
com.alibaba.dubbo.remoting.transport.netty4.NettyClient ，继承 AbstractNettyClient 抽象类，Netty 客户端实现类。

包括启动client和断开连接以及关闭连接。

### 50.4.2 NettyClientHandler ###
com.alibaba.dubbo.remoting.transport.netty4.NettyClientHandler ，实现 io.netty.channel.ChannelDuplexHandler 类，NettyClient 的处理器。

实现方式和serverHandler差不多，都是交给handler处理。

## 50.5 NettyBackedChannelBuffer ##
com.alibaba.dubbo.remoting.transport.netty4.NettyBackedChannelBuffer ，实现 ChannelBuffer 接口，基于 Netty ByteBuf 的 ChannelBuffer 实现类。

netty的ByteBuf接口的包装。每个实现方法都对应ByteBuf方法。

## 50.6 NettyCodecAdapter ##
com.alibaba.dubbo.remoting.transport.netty4.NettyCodecAdapter ，Netty 编解码适配器，将 Dubbo 编解码器 适配成 Netty4 的编码器和解码器。

**构造方法**
````
/** Netty 编码器 */
private final ChannelHandler encoder = new InternalEncoder();
/** Netty 解码器 */
private final ChannelHandler decoder = new InternalDecoder();
/** Dubbo 编解码器 */
private final Codec2 codec;
/** Dubbo URL */
private final URL url;
/** Dubbo ChannelHandler */
private final com.alibaba.dubbo.remoting.ChannelHandler handler;
public NettyCodecAdapter(Codec2 codec, URL url, com.alibaba.dubbo.remoting.ChannelHandler handler) {
    this.codec = codec;
    this.url = url;
    this.handler = handler;
}
````

### 50.6.1 InternalEncoder ###
io.netty.handler.codec.MessageToByteEncoder ，Netty4 编码器抽象类。

### 50.6.2 InternalDecoder ###
io.netty.handler.codec.ByteToMessageDecoder ，Netty4 解码器抽象类。

## 50.7 日志工厂 ##
自 2.2.1 开始，dubbo 开始内置 log4j、slf4j、jcl、jdk 这些日志框架的适配。

基本实现。
调用 NettyHelper#setNettyLoggerFactory() 方法，设置日志工厂，基于 Dubbo Logger 组件。

----

# 51 NIO 服务器（七）之 Netty3 实现 #
//  todo 等待完善


---- 

# 52 HTTP 服务器 #
在 dubbo-remoting-http 模块中实现，使用在 http://、 rest://、hessian://、webservice://协议。

dubbo-remoting-http 只提供 Server 部分

类结构：
- API 层：
	- 最外层：API 定义。
	- support 包： 公用实现。
- 实现层：
	- jetty 包：基于内嵌的 Jetty 实现，版本为 6.x 。
	- tomcat 包：基于内嵌的 Tomcat 实现，版本为 8.x 。
	- servlet 包：基于 Servlet Bridge Server 实现。简单的说，使用 war 包，部署在外部的 Tomcat 、Jetty 等 Servlet 容器。

- HttpBinder ，负责创建对应的 HttpServer 对象。
- 不同的 Protocol ，实现各自的 HttpHandler 类。并且，暴露服务时，启动 HttpServer 的同时，创建对应的 HttpHandler 对象，以 port 为键，注册到 DispatcherServlet 上。
- DispatcherServlet ，核心，调度请求，到对应的 HttpHandler 中。

整体流程图：

![](/picture/dubbo-http-flow.png)

## 52.1 API ##
### 52.1.1 HttpServer ###
com.alibaba.dubbo.remoting.http.HttpServer ，实现 Resetable 接口，HTTP 服务器接口。'

### 52.1.2 AbstractHttpServer ###
com.alibaba.dubbo.remoting.http.AbstractHttpServer ，实现 HttpServer 接口，HTTP 服务器抽象类。

### 52.1.3 HttpHandler ###
com.alibaba.dubbo.remoting.http.HttpHandler ，HTTP 处理器接口

### 52.1.4 HttpBinder ###
com.alibaba.dubbo.remoting.http.HttpBinder ，HTTP 绑定器接口。

````
@SPI("jetty")
public interface HttpBinder {
    /**
     * bind the server.
     */
    @Adaptive({Constants.SERVER_KEY})
    HttpServer bind(URL url, HttpHandler handler);
}
````

- @SPI("jetty") 注解，Dubbo SPI 拓展点，默认为 "jetty" ，即未配置情况下，使用 Jetty Server 。
- @Adaptive({Constants.SERVER_KEY}) 注解，基于 Dubbo SPI Adaptive 机制，加载对应的 Server 实现，使用 URL.server 属性。

### 52.1.5 DispatcherServlet ###
com.alibaba.dubbo.remoting.http.serlvet.DispatcherServlet ，实现 javax.servlet.http.HttpServlet 接口，服务请求调度 Servlet。

### 52.1.6 ServletManager ###
com.alibaba.dubbo.remoting.http.serlvet.ServletManager ，Servlet 管理器，负责管理 ServletContext

- EXTERNAL_SERVER_PORT 静态属性，外部服务器端口，用于 servlet 的服务器端口。
- contextMap 静态属性，ServletContext 集合。
	- addServletContext(port, ServletContext) 方法，添加。
	- removeServletContext(port) 方法，移除。
	- getServletContext(port) 方法，查询。

## 52.2 Tomcat 实现 ##

### 52.2.1 TomcatHttpServer ###
com.alibaba.dubbo.remoting.http.tomcat.TomcatHttpServer ，实现 AbstractHttpServer 抽象类，基于 Tomcat 的 HTTP 服务器实现类。

#### 52.2.1.1 构造方法 ####

````
/** 内嵌的 Tomcat 对象 */
private final Tomcat tomcat;
/** URL 对象 */
private final URL url;
public TomcatHttpServer(URL url, final HttpHandler handler) {
    super(url, handler);
    this.url = url;
    // 注册 HttpHandler 到 DispatcherServlet 中
    DispatcherServlet.addHttpHandler(url.getPort(), handler);
    // 创建内嵌的 Tomcat 对象
    String baseDir = new File(System.getProperty("java.io.tmpdir")).getAbsolutePath();
    tomcat = new Tomcat();
    tomcat.setBaseDir(baseDir);
    tomcat.setPort(url.getPort());
    tomcat.getConnector().setProperty("maxThreads", String.valueOf(url.getParameter(Constants.THREADS_KEY, Constants.DEFAULT_THREADS))); // 最大线程数
    tomcat.getConnector().setProperty("maxConnections", String.valueOf(url.getParameter(Constants.ACCEPTS_KEY, -1))); // 最大连接池
    tomcat.getConnector().setProperty("URIEncoding", "UTF-8"); // 编码为 UTF-8
    tomcat.getConnector().setProperty("connectionTimeout", "60000"); // 连接超时，60 秒
    tomcat.getConnector().setProperty("maxKeepAliveRequests", "-1");
    tomcat.getConnector().setProtocol("org.apache.coyote.http11.Http11NioProtocol");
    // 添加 DispatcherServlet 到 Tomcat 中
    Context context = tomcat.addContext("/", baseDir);
    Tomcat.addServlet(context, "dispatcher", new DispatcherServlet());
    context.addServletMapping("/*", "dispatcher");
    // 添加 ServletContext 对象，到 ServletManager 中
    ServletManager.getInstance().addServletContext(url.getPort(), context.getServletContext());
    // 启动 Tomcat
    try {
        tomcat.start();
    } catch (LifecycleException e) {
        throw new IllegalStateException("Failed to start tomcat server at " + url.getAddress(), e);
    }
}
````

基本流程如下：
1. 调用 DispatcherServlet#addHttpHandler(port, handler) 方法，注册 HttpHandler 到 DispatcherServlet 中。
1. 创建内嵌的 Tomcat 对象。
1. 创建并添加 DispatcherServlet 对象，到 Tomcat 中。
1. 调用 ServletManager#addServletContext(port, ServletContext) 方法，添加 DispatcherServlet 对象，到 ServletManager 中。
1. 调用 Tomcat#start() 方法，启动 Tomcat 。

#### 52.2.1.2 关闭 ####
````
@Override
public void close() {
    // 标记关闭
    super.close();

    // 移除 ServletContext 对象
    ServletManager.getInstance().removeServletContext(url.getPort());

    // 关闭 Tomcat
    try {
        tomcat.stop();
    } catch (Exception e) {
        logger.warn(e.getMessage(), e);
    }
}
````

## 52.3 Jetty 实现 ##
jetty 和 tomcat 包的实现，差不多，主要差异在 Tomcat 和 Jetty 的 API 不同。

## 52.4 Servlet Bridge 实现 ##
### 52.4.1 ServletHttpServer ###
com.alibaba.dubbo.remoting.http.servlet.ServletHttpServer ，实现 AbstractHttpServer 抽象类， 基于 Servlet 的服务器实现类。

- 注意，在 <dubbo:protocol /> 配置的端口，和外部的 Servlet 容器的端口，保持一致。
- 需要配置 DispatcherServlet 到 web.xml 中。通过这样的方式，让外部的 Servlet 容器，可以进行转发。

### 52.4.2 ServletHttpBinder ###


### 52.4.3 BootstrapListener ###
com.alibaba.dubbo.remoting.http.servlet.BootstrapListener ，实现 ServletContextListener 接口， 启动监听器。

需要配置 BootstrapListener 到 web.xml 中。通过这样的方式，让外部的 ServletContext 对象，添加到 ServletManager 中。

------

# 53 序列化（一）之总体实现 #
将对象转成字节流，用于网络传输，以及将字节流转为对象，用于在收到字节流数据后还原成对象。

序列化在 dubbo-common 项目的 serialize 模块实现。

## 53.1 API 定义 ##
### 53.1.1 Serialization ###
com.alibaba.dubbo.common.serialize.Serialization ，序列化接口。

- @SPI("hessian2") 注解，Dubbo SPI 拓展点，默认为 "hessian2" ，即未配置情况下，使用 Hessian 进行序列化和反序列化 。
- getContentTypeId(),#getContentType() 方法，获得内容类型编号和名字。

### 53.1.2 DataInput ###
com.alibaba.dubbo.common.serialize.DataInput ，数据输入接口。

从 InputStream 中，读取基本类型的数据。

#### 53.1.2.1 ObjectInput ####
com.alibaba.dubbo.common.serialize.ObjectInput ，实现 DataInput 接口，对象输入接口。

在 DataInput 的基础上，增加读取对象的数据。

### 53.1.3 DataOutput ###
DataOutput 和 DataInput 相反。

com.alibaba.dubbo.common.serialize.DataOutput ，数据输出接口。

向 InputStream 中，写入基本类型的数据。

#### 53.1.3.1 ObjectOutput ####
com.alibaba.dubbo.common.serialize.ObjectOutput ，实现 DataOutput 接口，对象输出接口。

在 DataOutput 的基础上，增加写入对象的数据。

### 53.1.4 Cleanable ###
com.alibaba.dubbo.common.serialize.Cleanable ，清理接口。
部分 Serialize 实现类，完成序列化或反序列化，需要做清理。通过实现该接口，执行清理的逻辑。

### 53.1.5 Optimizer 相关 ###
#### 53.1.5.1 SerializationOptimizer ####
com.alibaba.dubbo.common.serialize.support.SerializationOptimizer ，序列化优化器接口。

#### 53.1.5.2 SerializableClassRegistry ####
com.alibaba.dubbo.common.serialize.support.SerializableClassRegistry ，序列化优化类的注册表。

- registerClass(clazz) 静态方法，注册。在 SerializationOptimizer#getSerializableClasses() 方法，获得的类的集合，会注册到 SerializableClassRegistry 中。
- getRegisteredClasses() 静态方法，获得。在 Kryo 、FST 中，调用该方法，获得需要使用优化的类的集合。

#### 53.1.5.3 初始化序列化优化器 ####
在 DubboProtocol#optimizeSerialization() 方法中，初始化序列化优化器。

基本流程如下：
1. 获得 `optimizer` 配置项
2. 加载 SerializationOptimizer 实现类
3. 创建 SerializationOptimizer 对象
4. 注册到 SerializableClassRegistry 中
5. 添加到 optimizers 中

## 53.2 FST 实现 ##
### 53.2.1 FstFactory ###
com.alibaba.dubbo.common.serialize.support.fst.FstFactory ，FST 工厂。
- factory 静态属性，单例。
- conf 属性，FST 配置对象。在构造方法中，将 SerializableClassRegistry 注册表需要使用优化的类，注册到 FSTConfiguration 中。SerializableClassRegistry#registerClass(Class ... c) 方法
- getObjectOutput() 方法，获得 org.nustaq.serialization.FSTObjectOutput 对象，被 FstObjectOutput 调用。
- getObjectInput() 方法，获得 org.nustaq.serialization.FSTObjectInput 对象，被 FstObjectInput 调用。

### 53.2.2 FstSerialization ###
com.alibaba.dubbo.common.serialize.support.fst.FstSerialization ，实现 Serialization 接口，FST 序列化实现类。

- "x-application/fst" ，类似 HTTP 协议 的 Content-Types 的 Header 。

### 53.2.3 FstObjectInput ###
com.alibaba.dubbo.common.serialize.support.fst.FstObjectInput ，实现 ObjectInput 接口，FST 对象输入实现类。

**构造方法**

````
/**
 * input 属性，调用 FstFactory#getObjectInput(inputStream) 方法，获得。
 */ 
private FSTObjectInput input;
public FstObjectInput(InputStream inputStream) {
    input = FstFactory.getDefaultFactory().getObjectInput(inputStream);
}
````

**实现方法**
每个实现方法，直接调用 FSTObjectInput 对应的方法。

### 53.2.4 FstObjectOutput ###
com.alibaba.dubbo.common.serialize.support.fst.FstObjectOutput ，实现 ObjectOutput 接口，FST 对象输出实现类。

**构造方法**
````
/**
 * 调用 FstFactory#getObjectInput(outputStream) 方法，获得。
 */ 
private FSTObjectOutput output;

public FstObjectOutput(OutputStream outputStream) {
    output = FstFactory.getDefaultFactory().getObjectOutput(outputStream);
}
````

**实现方法**
每个实现方法，直接调用 FSTObjectInput 对应的方法。

## 53.3 JSON 实现 ##

基于 FastJSON 实现。

## 53.4 Hessian2 实现 ##
和其他 Web 服务的实现框架不同的是，Hessian 是一个使用二进制 Web 服务协议的框架，它的好处在于免除了一大堆附加的API包，例如 XML 的处理之类的 jar 包，这也就是为什么说它是一个轻量级的 Web 服务实现框架的原因，这个原因还在于手机上的应用程序可以通过 Hessian 提供的 API 很方便的访问 Hessian 的 Web 服务。

## 53.5 NativeJava 实现 ##

nativejava ，基于 Java 原生( 自带 )的 Java 序列化实现，即使用 java.io.ObjectInputStream 和 java.io.ObjectOutputStream 进行序列化和反序列化。

----

# 54 序列化（二）之 Dubbo 实现 #
Dubbo 自己实现的序列化拓展。要实现序列化的高性能，需要考虑两方面：
- 序列化和反序列化速度快
- 从传输角度，数据压缩效果好，即序列化后的数据量体积小

## 54.1 GenericDataFlags ##
Dubbo ，是一种有序、紧凑的序列化方式。如下是序列化后的二进制数据流的示意图：

![](/picture/dubbo-serialize-data.png)

不同于 JSON / XML 等序列化方式，无需序列化每个属性名。通过 Builder 对象，创建每个类的序列化和反序列化的具体代码。
- 这样，我们就避免了属性名的序列化，提升了速度，减少了数据的体积。
- 当然，反过来说，如果对象发生了变化( 增加或删除属性 )，可能会出现 Client 和 Server 序列化的不兼容，因为属性的顺序发生了变化。
- 
属性值和属性值之间无间隔，通过属性值的标志位 Flag 保证

com.alibaba.dubbo.common.serialize.support.dubbo.GenericDataFlags 通用数据标记枚举规整，如下图所示：
![](/picture/dubbo-GenericDataFlags.png)

- 在每个属性值( 即 field )的首个 Byte 位，称为标志位 Flag 。目前我们分成两大类( 图中，绿色部分 )：
	- Varint ，变长数字，占用 Byte 值的 [0, 128) 区间。
	- Object ，对象，占用 Byte 值的 [-128, 0) 区间。
- 标志位 Flag 根据用途，可以分成两种类型（注意，值是不重叠的）：
	- Tag ，标签( 图中，橙色部分 )。
		- 以 VarInt 举例子，数字分成 BYTE、SHORT、INT、LONG 四种数据类型。 通过标记位，表示数字占用多少 Byte ，从而实现变长，节省 Byte 的占用。例如，属性值类型为 Long ，但是值是 100L ，那么只需要要 1 Byte( 标记位为 VARINT8 ) + 1 Byte( 100L ) = 2 Byte 。
		- 当然，这种方式也有缺点，对于大整数，会多占用一个标记位，例如 Integer.MAX_VALUE 。从统计上来说，业务系统更多的是小整数。所以，这个缺点也是能够接受的。
	- CONSTANTS ， 枚举( 图中，黄色部分 )，用于常用属性值。
		- 以 Varint 举例子，在业务系统中，[ -15, 31 ] 是非常常用。通过枚举，进一步减少数据提及，提升序列化速度。所以 Varint 的二进制数据流示意图如下：

![](/picture/dubbo-Varint-byte.png)

## 54.2 Data ##
### 54.2.1 GenericDataOutput ###
com.alibaba.dubbo.common.serialize.support.dubbo.GenericDataOutput ，实现 DataOutput，GenericDataFlags 接口，Dubbo 数据输出实现类。
#### 54.2.1.1 构造方法 ####

````
/**
 * 默认 {@link #mCharBuf} 大小
 */
private static final int CHAR_BUF_SIZE = 256;
/**
 * 序列化字符串的临时结果的 Buffer 数组，用于 {@link #writeUTF(String)} 中。
 */
private final char[] mCharBuf = new char[CHAR_BUF_SIZE];

/**
 * 序列化 Varint 的临时结果的 Buffer 数组，用于 {@link #writeVarint32(int)} 和 {@link #writeVarint64(long)} 中。
 */
private final byte[] mTemp = new byte[9];

/**
 * 序列化结果的 Buffer 数组
 */
private final byte[] mBuffer;
/**
 * {@link #mBuffer} 容量大小
 */
private final int mLimit;
/**
 * {@link #mBuffer} 当前写入位置
 */
private int mPosition = 0;

/**
 * 结果输出
 */
private final OutputStream mOutput;
````

----


# 55 序列化（三）之 Kryo 实现 #

# 56 服务容器 #

- 服务容器是一个 standalone 的启动程序，因为后台服务不需要 Tomcat 或 JBoss 等 Web 容器的功能，如果硬要用 Web 容器去加载服务提供方，增加复杂性，也浪费资源。
- 服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。
- 服务容器的加载内容可以扩展，内置了 spring, jetty, log4j 等加载，可通过容器扩展点进行扩展。配置配在 java 命令的 -D 参数或者 dubbo.properties 中。

## 56.1 Container ##
com.alibaba.dubbo.container.Container ，服务容器接口。

- @SPI("spring") 注解，Dubbo SPI 拓展点，默认为 "spring" 。
- 定义了容器的启动和停止两个方法。

### 56.1.1 SpringContainer ###
com.alibaba.dubbo.container.spring.SpringContainer ，实现 Container 接口，Spring 容器实现类。

````
public class SpringContainer implements Container {
    private static final Logger logger = LoggerFactory.getLogger(SpringContainer.class);
    /**
     * Spring 配置属性 KEY
     */
    public static final String SPRING_CONFIG = "dubbo.spring.config";
    /**
     * 默认配置文件地址
     */
    public static final String DEFAULT_SPRING_CONFIG = "classpath*:META-INF/spring/*.xml";
    /**
     * Spring Context
     * 静态属性，全局唯一
     */
    static ClassPathXmlApplicationContext context;
    public static ClassPathXmlApplicationContext getContext() {
        return context;
    }
    @Override
    public void start() {
        // 获得 Spring 配置文件的地址
        String configPath = ConfigUtils.getProperty(SPRING_CONFIG);
        if (configPath == null || configPath.length() == 0) {
            configPath = DEFAULT_SPRING_CONFIG;
        }
        // 创建 Spring Context 对象
        context = new ClassPathXmlApplicationContext(configPath.split("[,\\s]+"));
        // 启动 Spring Context ，会触发 ContextStartedEvent 事件
        context.start();
    }
    @Override
    public void stop() {
        try {
            if (context != null) {
                // 停止 Spring Context ，会触发 ContextStoppedEvent 事件。
                context.stop();
                // 关闭 Spring Context ，会触发 ContextClosedEvent 事件。
                context.close();
                context = null;
            }
        } catch (Throwable e) {
            logger.error(e.getMessage(), e);
        }
    }
}
````

- start() 方法，启动 Spring 。
1. 调用 ConfigUtils#getProperty(key) 方法，获得 Spring 配置文件的地址。优先级为：
	1. 【高】JVM 启动参数：-Ddubbo.spring.config=自定义 XML 路径 。
	2. 【低】Dubbo Properties 配置文件：dubbo.spring.config=自定义 XML 路径 。
2. 未配置，则使用默认路径 DEFAULT_SPRING_CONFIG 。
3. 创建 Spring Context 对象。
4. 调用 ClassPathXmlApplicationContext#start() 方法，启动 Spring Context 。通过 Spring 启动，加载我们的 Dubbo 配置，从而启动 Dubbo 服务。
- stop() 方法，关闭 Spring 。
1. 调用 ClassPathXmlApplicationContext#stop() 方法，停止 Spring Context 。
2. 调用 ClassPathXmlApplicationContext#close() 方法，关闭 Spring Context 。

----

# 57 集群容错（一）之抽象 API #
## 57.1 Cluster ##
com.alibaba.dubbo.rpc.cluster.Cluster ，集群接口。

- @SPI(FailoverCluster.NAME) 注解，Dubbo SPI 拓展点，默认为 "failover" ，即失败重试，也就是会贯穿本文的 FailoverCluster 类。
- @Adaptive 注解，基于 Dubbo SPI Adaptive 机制，加载对应的 Cluster 实现，使用 URL.cluster 属性。
- join(Directory<T>) 接口方法，基于 Directory ，创建 Invoker 对象，实现统一、透明的 Invoker 调用过程。

### 57.1.1 join 方法 ###
在 RegistryProtocol 的 #doRefer(Cluster, Registry, type, url) 方法中，会调用 Cluster#join(directory) 方法，创建 Invoker 对象。

## 57.2 FailoverCluster ##
com.alibaba.dubbo.rpc.cluster.support.FailoverCluster ，实现 Cluster 接口，失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries="2" 来设置重试次数(不含第一次)。
对应 Invoker 为 FailoverClusterInvoker 。

## 57.3 AbstractClusterInvoker ##
com.alibaba.dubbo.rpc.cluster.support.AbstractClusterInvoker ，实现 Invoker 接口，Cluster Invoker 抽象类：

- 实现例如选择一个符合 Invoker 对象等等公用方法
- 定义 #doInvoke(Invocation, List<Invoker<T>>, LoadBalance) 抽象方法，实现子 Cluster 的 Invoker 实现类的服务调用的差异逻辑，代码如下：

````
protected abstract Result doInvoke(Invocation invocation, List<Invoker<T>> invokers, LoadBalance loadbalance) throws RpcException;
````
### 57.3.1 构造方法 ###

````
/**
 * Directory 对象。通过它，可以获得所有服务提供者的 Invoker 对象。
 */
protected final Directory<T> directory;
/**
 * 集群时是否排除非可用( available )的 Invoker ，默认为 true.通过 "cluster.availablecheck" 配置项设置。
 */
protected final boolean availablecheck;
/**
 * 是否已经销毁,若已经销毁，则不允许在调用。
 */
private AtomicBoolean destroyed = new AtomicBoolean(false);
/**
 * 粘滞连接 Invoker
 *
 * https://dubbo.gitbooks.io/dubbo-user-book/demos/stickiness.html
 * 粘滞连接用于有状态服务，尽可能让客户端总是向同一提供者发起调用，除非该提供者挂了，再连另一台。
 * 粘滞连接将自动开启延迟连接，以减少长连接数。
 */
private volatile Invoker<T> stickyInvoker = null;

public AbstractClusterInvoker(Directory<T> directory) {
    this(directory, directory.getUrl());
}

public AbstractClusterInvoker(Directory<T> directory, URL url) {
    // 初始化 directory
    if (directory == null) {
        throw new IllegalArgumentException("service directory == null");
    }
    this.directory = directory;
    // sticky: invoker.isAvailable() should always be checked before using when availablecheck is true.
    // 初始化 availablecheck
    this.availablecheck = url.getParameter(Constants.CLUSTER_AVAILABLE_CHECK_KEY, Constants.DEFAULT_CLUSTER_AVAILABLE_CHECK);
}
````

### 57.3.2 list ###
list(Invocation) 方法，获得所有服务提供者 Invoker 集合

### 57.3.3 select ###
select(LoadBalance, Invocation, invokers, selected) 方法，从候选的 Invoker 集合，选择一个最终调用的 Invoker 对象。

该方法主要处理粘滞连接的特性，具体使用 Loadbalance 选择 Invoker 对象的逻辑，在 #doselect(loadbalance, invocation, invokers, selected) 方法中。

基本流程如下：
1. 获得粘滞连接 stickyInvoker 对象。
	1. 获得方法级的 sticky 配置项。
	2. 若 stickyInvoker 不存在于 invokers 中，说明不在候选中，需要置空，重新选择。
	3. 获得粘滞连接 stickyInvoker 对象。如要满足如下条件：
		1. 1）开启粘滞连接的特性；2）stickyInvoker 不存在于 selected 中。
		2. 若开启排除非可用的 Invoker 的特性，则校验 stickyInvoker 是否可用。
	4. 调用 #doselect(loadbalance, invocation, invokers, selected) 方法，执行选择一个 Invoker 对象。
	5. 若开启粘滞连接的特性，记录最终选择的 Invoker 对象，到 stickyInvoker 中。

#### 57.3.3.1 doselect ####
doselect(loadbalance, invocation, invokers, selected) 方法，从候选的 Invoker 集合，选择一个最终调用的 Invoker 对象。

有五种选择最终调用的 Invoker 对象的方式。
- 【第一种】如果只有一个候选的 Invoker 对象，直接选择返回。
- 【第二种】如果只有两个候选的 Invoker 集合，退化为轮询。
- 【第三种】调用 Loadbalance#select(invokers, url, invocation) 方法，使用 Loadbalance ，选择一个 Invoker 对象。	
	- 这种方式的返回，选择的 Invoker 对象，需要满足两个条件：1）不存在于 selected 中。2）Invoker 是可用的，若开启排除非可用的 Invoker 的特性。
- 【第四种】调用 #reselect(loadbalance, invocation, invokers, selected, availablecheck) 方法，重新选择一个 Invoker 对象。因为此时 invokers 中，无法找到一个满足条件的 Invoker 对象。
- 【第五种】顺序从候选的 invokers 集合中，选择一个 Invoker 对象，不考虑是否可用，又或者已经选择过，类似【第一种】【第二种】的方式。

#### 57.3.3.2 reselect ####
reselect(loadbalance, invocation, invokers, selected, availablecheck) 方法，重新选择一个 Invoker 对象。

1. 预先创建一个重选 Invoker 集合
	- 一共有两类三种的选择方式：
		- 获得非选择过( invokers )， 并且必须可用的 Invoker 集合。
		- 获得非选择过( invokers )， 并且不考虑可用的 Invoker 集合。
		- 选择过( selected )，并且必须可用的 Invoker 集合。
2. 调用 Loadbalance#select(invokers, url, invocation) 方法，使用 Loadbalance ，选择一个 Invoker 对象。

### 57.3.4 invoke ###
invoke(invocation) 方法，调用服务提供者的逻辑。

基本逻辑如下：
1. 调用 #checkWhetherDestroyed() 方法，校验是否已经销毁。
	1. 调用 #list(invocation) 方法，基于 Directory ，获得所有服务提供者 Invoker 集合。
	2. 获得 Loadbalance 对象。
3. 调用 RpcUtils#attachInvocationIdIfAsync(url, invocation) 方法，设置调用编号，若是异步调用。
4. 调用 #doInvoke(invocation, invokers, loadbalance) 抽象方法，执行调用。

## 57.4 FailoverClusterInvoker ##
com.alibaba.dubbo.rpc.cluster.support.FailoverClusterInvoker ，实现 AbstractClusterInvoker 抽象类，FailoverCluster Invoker 实现类。
失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries="2" 来设置重试次数(不含第一次)。

时序图：

![](/picture/dubbo-failover-flow.png)

实际逻辑很简单：循环，查找一个 Invoker 对象，进行调用，直到成功。

doInvoke(Invocation, List<Invoker<T>>, LoadBalance) 方法

基本流程如下：
1. 候选的 Invoker 集合。
2. 调用父 #checkInvokers(copyinvokers, invocation) 方法，校验候选的 Invoker 集合非空。如果为空，抛出 RpcException 异常。
3. 获得最大可调用次数：最大可重试次数 +1 。默认最大可重试次数Constants.DEFAULT_RETRIES = 2 。
4. le 变量，保存最后一次调用的异常。
5. invoked 变量，保存已经调用的 Invoker 集合。
6. providers 变量，保存已经调用的网络地址集合。
7. failover 机制核心实现：如果出现调用失败，那么重试其他服务器。
	1. 重试时( i > 0 )， 进行重新选择，避免重试时，候选 Invoker 集合，已发生变化。
	2. 调用父 #select(loadbalance, invocation, copyinvokers, invoked) 方法，根据 Loadbalance 负载均衡机制，从 copyinvokers 中，选择一个被调用的 Invoker 对象。
	3. 保存每次调用的 Invoker 对象，到 invoked 中。
	4. 保存已经调用的 Invoker 集合，到 Context 中。
	5. 调用 Invoker#invoke(invocation) 方法，发起 RPC 调用。
	6. 若 le 非空，说明此时是重试调用成功，将最后一次调用的异常信息以 warn 级别日志输出，方便未来追溯。
	7. 如果是业务性质的异常，不再重试，直接抛出。
	8. 保存异常到 le 。
	9. 非 RpcException 异常，封装成 RpcException 异常。
	10. 保存每次调用的网络地址，到 providers 中。
8. 超过最大调用次数，抛出 RpcException 异常。该异常中，带有最后一次调用异常的信息。

----

# 58 集群容错（二）之 Cluster 实现 #

## 58.1 AvailableCluster ##
com.alibaba.dubbo.rpc.cluster.support.AvailableCluster ，实现 Cluster 接口，调用首个可用服务器，目前用于多注册中心引用。

对应 Invoker 实现类为 AvailableClusterInvoker 。

### 58.1.1AvailableClusterInvoker ###
com.alibaba.dubbo.rpc.cluster.support.AvailableClusterInvoker ，实现 AbstractClusterInvoker 抽象类，AvailableCluster Invoker 实现类。

## 58.2 BroadcastCluster ##
com.alibaba.dubbo.rpc.cluster.support.BroadcastCluster ，实现 Cluster 接口，广播调用所有提供者，逐个调用，任意一台报错则报错。通常用于通知所有提供者更新缓存或日志等本地资源信息。

对应 Invoker 实现类为 BroadcastClusterInvoker 。

### 58.2.1 BroadcastClusterInvoker ###
com.alibaba.dubbo.rpc.cluster.support.BroadcastClusterInvoker ，实现 AbstractClusterInvoker 抽象类，BroadcastCluster Invoker 实现类。

## 58.3 FailbackCluster ##
com.alibaba.dubbo.rpc.cluster.support.FailbackCluster ，实现 Cluster 接口，失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。

对应 Invoker 实现类为 FailbackClusterInvoker 。

### 58.4.1 FailbackClusterInvoker ###
com.alibaba.dubbo.rpc.cluster.support.FailbackClusterInvoker ，实现 AbstractClusterInvoker 抽象类，FailbackCluster Invoker 实现类。

#### 58.4.1.1 构造方法 ####
````
/** 重试频率 */
private static final long RETRY_FAILED_PERIOD = 5 * 1000;

/** ScheduledExecutorService 对象 */
private final ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(2, new NamedThreadFactory("failback-cluster-timer", true));
/** 失败任务集合 */
private final ConcurrentMap<Invocation, AbstractClusterInvoker<?>> failed = new ConcurrentHashMap<Invocation, AbstractClusterInvoker<?>>();
/** 重试任务 Future */
private volatile ScheduledFuture<?> retryFuture;

public FailbackClusterInvoker(Directory<T> directory) {
    super(directory);
}
````
#### 58.4.1.2 doInvoke ####

若 RPC 调用失败，则调用 #addFailed(invocation, this) 方法，添加到 failed 中，后台定时重试。

#### 58.4.1.3 addFailed ####
创建的定时任务，会调用 #retryFailed() 方法，重试任务，发起 RCP 调用。

#### 58.4.1.4 retryFailed ####
循环重试任务，逐个发起 RPC 调用。若调用成功，移除该失败任务出 failed 集合。

## 58.5 FailfastCluster ##
com.alibaba.dubbo.rpc.cluster.support.FailfastCluster ，实现 Cluster 接口，快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。

对应 Invoker 实现类为 FailfastClusterInvoker 。

### 58.5.1 FailfastInvoker ###
com.alibaba.dubbo.rpc.cluster.support.FailbackClusterInvoker ，实现 AbstractClusterInvoker 抽象类，Failfast Invoker 实现类。

和 FailbackClusterInvoker 差异点，在于对异常的处理。



## 58.6 FailsafeCluster ##
com.alibaba.dubbo.rpc.cluster.support.FailsafeCluster ，实现 Cluster 接口，失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。

对应 Invoker 实现类为 FailsafeClusterInvoker 。

### 58.6.2 FailsafeClusterInvoker ###
com.alibaba.dubbo.rpc.cluster.support.FailsafeClusterInvoker ，实现 AbstractClusterInvoker 抽象类，Failsafe Invoker 实现类。

和 FailfastInvoker 差异点，在于对异常的处理。

## 58.6 ForkingCluster ##
com.alibaba.dubbo.rpc.cluster.support.ForkingCluster ，实现 Cluster 接口，并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks="2" 来设置最大并行数。

### 58.6.1 ForkingClusterInvoker ###
com.alibaba.dubbo.rpc.cluster.support.ForkingClusterInvoker ，实现 AbstractClusterInvoker 抽象类，ForkingCluster Invoker 实现类。

----

# 59 集群容错（三）之 Directory 实现 #
Directory ，中文直译为目录，代表了多个 Invoker ，可以把它看成 List<Invoker> 。但与 List 不同的是，它的值可能是动态变化的，比如注册中心推送变更。

- StaticDirectory ，静态 Directory 实现类，从命名上看出它是静态的 List<Invoker> 。
- RegistryDirectory ，基于注册中心的动态 Directory 实现类，从命名上看出它是动态的，会根据注册中心的推送变更 List<Invoker> 。

## 59.1 Directory ##
com.alibaba.dubbo.rpc.cluster.Directory ，继承 Node 接口，Directory 接口

定义了两个接口方法，分别返回服务的类型和 Invoker 集合。
一个 Directory 只对应一个服务类型。
## 59.2 AbstractDirectory ##

com.alibaba.dubbo.rpc.cluster.directory.AbstractDirectory ，实现 Directory 接口，Directory 抽象实现类，实现了公用的路由规则( Router )的逻辑。

### 59.2.1 构造方法 ###
````
/** 是否已经销毁 */
private volatile boolean destroyed = false;
/** 注册中心 URL */
private final URL url;
/**
 * 消费者 URL
 *
 * 若未显示调用 {@link #AbstractDirectory(URL, URL, List)} 构造方法，consumerUrl 等于 {@link #url}
 */
private volatile URL consumerUrl;
/** Router 数组 */
private volatile List<Router> routers;

public AbstractDirectory(URL url) {
    this(url, null);
}

public AbstractDirectory(URL url, List<Router> routers) {
    this(url, url, routers);
}

public AbstractDirectory(URL url, URL consumerUrl, List<Router> routers) {
    if (url == null) {
        throw new IllegalArgumentException("url == null");
    }
    this.url = url;
    this.consumerUrl = consumerUrl;
    // 设置 Router 数组
    setRouters(routers);
}
````

调用 #setRouters(routers) 方法，初始化并设置 Router 数组。
### 59.2.2 setRouters ###
setRouters(routers) 方法，初始化并设置 Router 数组。

### 59.2.3 list ###
list(Invocation) 实现方法，获得所有服务 Invoker 集合。

基本流程如下：
1. 获得所有 Invoker 集合
2. 根据路由规则，筛选 Invoker 集合

## 59.3 RegistryDirectory ##
com.alibaba.dubbo.registry.integration.RegistryDirectory ，实现 NotifyListener 接口，实现 AbstractDirectory 抽象类，基于注册中心的 Directory 实现类。

- RegistryDirectory 在 dubbo-registry 模块，integration 包下，是 Dubbo 注册中心模块集成 Directory 的实现类。
- RegistryDirectory 作为一个 NotifyListener ，订阅注册中心( Registry ) 的数据，实现对变更的监听。

### 59.3.1 构造方法 ###

````
// ========== Dubbo SPI Adaptive 对象 BEGIN ==========

/**
 * Cluster$Adaptive 对象
 */
private static final Cluster cluster = ExtensionLoader.getExtensionLoader(Cluster.class).getAdaptiveExtension();
/**
 * RouterFactory$Adaptive 对象
 */
private static final RouterFactory routerFactory = ExtensionLoader.getExtensionLoader(RouterFactory.class).getAdaptiveExtension();
/**
 * ConfiguratorFactory$Adaptive 对象
 */
private static final ConfiguratorFactory configuratorFactory = ExtensionLoader.getExtensionLoader(ConfiguratorFactory.class).getAdaptiveExtension();

// ========== 服务消费者相关 BEGIN ==========

/**
 * 服务类型，例如：com.alibaba.dubbo.demo.DemoService
 */
private final Class<T> serviceType; // Initialization at construction time, assertion not null
/**
 * Consumer URL 的配置项 Map
 */
private final Map<String, String> queryMap; // Initialization at construction time, assertion not null
/**
 * 服务方法数组
 */
private final String[] serviceMethods;
/**
 * 是否引用多分组
 *
 * 服务分组：https://dubbo.gitbooks.io/dubbo-user-book/demos/service-group.html
 */
private final boolean multiGroup;

// ========== 注册中心相关 BEGIN ==========

/**
 * 注册中心的 Protocol 对象
 */
private Protocol protocol; // Initialization at the time of injection, the assertion is not null
/**
 * 注册中心
 */
private Registry registry; // Initialization at the time of injection, the assertion is not null
/**
 * 注册中心的服务类，目前是 com.alibaba.dubbo.registry.RegistryService
 *
 * 通过 {@link #url} 的 {@link URL#getServiceKey()} 获得
 */
private final String serviceKey; // Initialization at construction time, assertion not null
/**
 * 是否禁止访问。
 *
 * 有两种情况会导致：
 *
 * 1. 没有服务提供者
 * 2. 服务提供者被禁用
 */
private volatile boolean forbidden = false;

// ========== 配置规则相关 BEGIN ==========

/**
 * 原始的目录 URL
 *
 * 例如：zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-consumer&callbacks=1000&check=false&client=netty4&cluster=failback&dubbo=2.0.0&interface=com.alibaba.dubbo.demo.DemoService&methods=sayHello,callbackParam,save,update,say03,delete,say04,demo,say01,bye,say02,saves&payload=1000&pid=63400&qos.port=33333&register.ip=192.168.16.23&sayHello.async=true&side=consumer&timeout=10000&timestamp=1527056491064
 */
private final URL directoryUrl; // Initialization at construction time, assertion not null, and always assign non null value
/**
 * 覆写的目录 URL ，结合配置规则
 */
private volatile URL overrideDirectoryUrl; // Initialization at construction time, assertion not null, and always assign non null value
/**
 * 配置规则数组
 *
 * override rules
 * Priority: override>-D>consumer>provider
 * Rule one: for a certain provider <ip:port,timeout=100>
 * Rule two: for all providers <* ,timeout=5000>
 */
private volatile List<Configurator> configurators; // The initial value is null and the midway may be assigned to null, please use the local variable reference

// ========== 服务提供者相关 BEGIN ==========

/**
 * [url]与[服务提供者 Invoker 集合]的映射缓存
 */
// Map<url, Invoker> cache service url to invoker mapping.
private volatile Map<String, Invoker<T>> urlInvokerMap; // The initial value is null and the midway may be assigned to null, please use the local variable reference
/**
 * [方法名]与[服务提供者 Invoker 集合]的映射缓存
 */
// Map<methodName, Invoker> cache service method to invokers mapping.
private volatile Map<String, List<Invoker<T>>> methodInvokerMap; // The initial value is null and the midway may be assigned to null, please use the local variable reference
/**
 * [服务提供者 Invoker 集合]缓存
 */
// Set<invokerUrls> cache invokeUrls to invokers mapping.
private volatile Set<URL> cachedInvokerUrls; // The initial value is null and the midway may be assigned to null, please use the local variable reference

public RegistryDirectory(Class<T> serviceType, URL url) {
    super(url);
    if (serviceType == null) {
        throw new IllegalArgumentException("service type is null.");
    }
    if (url.getServiceKey() == null || url.getServiceKey().length() == 0) {
        throw new IllegalArgumentException("registry serviceKey is null.");
    }
    this.serviceType = serviceType;
    this.serviceKey = url.getServiceKey();
    // 获得 queryMap
    this.queryMap = StringUtils.parseQueryString(url.getParameterAndDecoded(Constants.REFER_KEY));
    // 获得 overrideDirectoryUrl 和 directoryUrl
    this.overrideDirectoryUrl = this.directoryUrl = url.setPath(url.getServiceInterface()).clearParameters().addParameters(queryMap).removeParameter(Constants.MONITOR_KEY);
    // 初始化 multiGroup
    String group = directoryUrl.getParameter(Constants.GROUP_KEY, "");
    this.multiGroup = group != null && ("*".equals(group) || group.contains(","));
    // 初始化 serviceMethods
    String methods = queryMap.get(Constants.METHODS_KEY);
    this.serviceMethods = methods == null ? null : Constants.COMMA_SPLIT_PATTERN.split(methods);
}
````

### 59.3.2 subscribe ###
subscribe(URL) 方法，向注册中心发起订阅。

### 59.3.3 notify ###
在注册中心( Registry )发现数据发生变化时，会通知对应的 NotifyListener 们。

notify(List<URL> urls) 实现方法
1. 根据 URL 的分类或协议，分成组三个集合：1）服务提供者 + 2）路由规则 + 3）配置规则。
2. 非空，调用 #toConfigurators(configuratorUrls) 方法，处理配置规则 URL 集合。
3. 非空，调用 #toRouters(routerUrls) 方法，处理路由规则 URL 集合。
	1. 若转换到 routers 非空，调用父 #setRouters(routers) 方法，设置路由规则。
2. 合并配置规则，到 directoryUrl 中，形成 overrideDirectoryUrl 变量。
3. 调用 #refreshInvoker(invokerUrls) 方法，处理服务提供者 URL 集合。

### 59.3.4 内部类 ###
#### 59.3.4.1 InvokerDelegate ####
InvokerDelegate ，实现 com.alibaba.dubbo.rpc.protocol.InvokerWrapper 类，Invoker 代理类，主要用于存储注册中心下发的 url 地址( providerUrl )，用于重新重新 refer 时能够根据 providerURL queryMap overrideMap 重新组装。

#### 59.3.4.2 InvokerComparator ####
InvokerComparator ，实现 Comparator 接口，Invoker 排序器实现类，根据 URL 升序 。

#### 59.3.4.3 refreshInvoker ####
refreshInvoker(List<URL> invokerUrls) 方法，官方注释其如下
根据 invokerURL 列表转换为 invoker 列表。转换规则如下：

- 如果 url 已经被转换为 invoker ，则不在重新引用，直接从缓存中获取，注意如果 url 中任何一个参数变更也会重新引用
- 如果传入的 invoker 列表不为空，则表示最新的 invoker 列表
- 如果传入的 invokerUrl 列表是空，则表示只是下发的 override 规则或 route 规则，需要重新交叉对比，决定是否需要重新引用。

基本逻辑如下：

**第一部分**
1. 当 invokerUrls 集合大小为 1 ，并且协议为 empty:// ，说明所有服务提供者都已经下线。若注册中心为 Zookeeper ，可参见 ZookeeperRegistry#toUrlsWithEmpty(URL consumer, String path, List<String> providers) 方法。
2. 设置禁止访问，因为没有服务提供者了。
3. methodInvokerMap 置空。
4. 调用 #destroyAllInvokers() 方法，销毁所有服务提供者 Invoker 集合。

**第二部分**
1. 设置允许访问，因为有服务提供者了。
2. 传入的 invokerUrls 为空，说明是路由规则或配置规则发生改变，此时 invokerUrls 是空的，直接使用 cachedInvokerUrls 。对应官方注释【第 3 点】（部分，不包括“需要重新交叉对比，决定是否需要重新引用”）。
3. 传入的 invokerUrls 非空，更新 cachedInvokerUrls 。考虑到并发的问题，更新的方式为创建新的 HashSet 。对应官方注释【第 2 点】。
4. 忽略，若无 invokerUrls 。出现情况为，初始是按照 configurators => routers => providers ，所以前两个会出现这个情况。
5. 调用 #toInvokers(List<URL> urls) 方法，将传入的 invokerUrls ，转换成新的 urlInvokerMap 。
6. 调用 #toMethodInvokers(newUrlInvokerMap) 方法，将 urlInvokerMap 转成与方法的映射关系，即新的 methodInvokerMap 。
7. 如果计算错误，则不进行处理。一般来说，是防御性编程。
8. 若服务引用多 group ，则调用 #toMergeMethodInvokerMap(newMethodInvokerMap) 方法，按照 method + group 聚合 Invoker 集合。
9. 赋值 urlInvokerMap 属性。
10. 调用 #destroyUnusedInvokers(oldUrlInvokerMap, newUrlInvokerMap) 方法，销毁不再使用的 Invoker 集合。

##### 59.3.4.3.1 toInvokers #####
将传入的urls转换成urlInvokerMap

基本流程如下：
1. newUrlInvokerMap 变量，新的 urlInvokerMap 字段，后面会赋值给它。
2. 若 urls 为空，直接返回，防御性编程。
3. 已初始化的服务器提供 URL 集合，即服务提供者 URL 已经处理。
4. 获得引用服务的协议。一般情况下，我们不会设置 <dubbo:reference protocol=""/> 配置项。
5. 循环 urls 集合，转成 Invoker 集合。
6. 协议处理
	1. 如果 reference 端配置了 protocol ，则只选择匹配的 protocol 。
	2. 忽略，若为 empty:// 协议。
	3. 忽略，若应用程序不支持该协议。
4. 调用 #mergeUrl(providerUrl) 方法，合并 URL 参数。
5. 忽略，通过 keys 判断已经初始化。
	1. 若未初始化，添加到 keys 中。
2. “创建”服务 Invoker 对象。
	1. 获得 url 对应在 localUrlInvokerMap 缓存的 Invoker 对象。
	2. 不在缓存中，需要重新 refer 引用，创建 Invoker 对象。
		1. 通过配置项 enable 和 disable 判断，服务是否开启。
		2. 若开启，创建 Invoker 对象。
			1. 调用 Protocol$Adaptive#refer(serviceType, url) 方法，引用服务，创建服务提供者 Invoker 对象。
			2. 创建 InvokerDelegate 对象。
	3. 在缓存中，直接使用缓存的 Invoker 对象，添加到 newUrlInvokerMap 中。
4. 清空 keys 。
5. 返回结果 newUrlInvokerMap 。

###### 59.3.4.3.1 mergeUrl ######
mergeUrl(providerUrl) 方法，合并 URL 参数，优先级为配置规则 > 服务消费者配置 > 服务提供者配置。

基本流程如下：
1. 调用 ClusterUtils#mergeUrl(providerUrl, queryMap) 方法，合并服务消费者配置到 providerUrl 
2. 设置 providerUrl 不检查连接是否成功，总是创建 Invoker 
3. 仅合并提供者参数。

###### 59.3.4.3.2 toMethodInvokers ######
toMethodInvokers(Map<String, Invoker<T>> invokersMap) 方法，将 invokersMap 转成与方法的映射关系。

基本流程如下：
1. newMethodInvokerMap 变量，新的 methodInvokerMap 字段，后面会赋值给它。
1. 创建 Invoker 集合。实际就是 invokersMap 的值的集合。
1. 按照方法名为维度( KEY ) ，聚合对应的 Invoker 集合到 newMethodInvokerMap 中。
1. 路由全 invokersList ，匹配合适的 Invoker 集合。
1. 添加 newInvokersList 到 newMethodInvokerMap 中，表示该服务提供者的全量 Invoker 集合。
1. 循环，基于每个方法路由，匹配合适的 Invoker 集合。
1. 循环排序每个方法的 Invoker 集合，并设置为不可变。

##### 59.3.4.4 toMergeMethodInvokerMap #####
toMergeMethodInvokerMap(Map<String, List<Invoker<T>>> methodMap) ，按照 method + group 聚合 Invoker 集合。

result 属性，新的 methodInvokerMap 字段，后面会赋值给它。

基本流程：
1. 循环，按照 method + group 聚合 Invoker 集合。	
	1. 按照 Group 聚合 Invoker 集合的结果。其中，KEY：group ，VALUE：Invoker 集合。
	2. 循环 Invoker 集合，按照 group 聚合 Invoker 集合。
	3. 若数量为 1 ，使用第一个。
	4. 若数量为 0 ，使用原有值 invokers 。
	5. 若数量大于 1 ，循环每个 Group 的 Invoker 集合，调用 Cluster$Adaptive#join(Directory) 方法，创建对应的 Cluster Invoker 对象。

##### 59.3.4.4.1 destroyUnusedInvokers #####
destroyUnusedInvokers(oldUrlInvokerMap, newUrlInvokerMap) 方法，销毁不再使用的 Invoker 集合。

##### 59.3.4.4.2 destroyAllInvokers #####
destroyAllInvokers() 方法，销毁所有服务提供者 Invoker 

### 59.3.5 doList ###
doList(Invocation) 实现方法，获得对应的 Invoker 集合。

通过四种方式，从 methodInvokerMap 中，获得对应的 Invoker 集合。
- 第一种，可根据第一个参数枚举路由。这是个非常小众的场景，
	- 通过这样的方式，调用到的服务提供者的 DemoServiceImpl#hello01(name) 方法。
	- 如果使用该特性，注意避免出现无关的几个方法，例如 #hello(name) 和 #hello01(name) 是毫无关系的两个方法，而我真的想调用 #hello(name) 方法，结果调用到了 #hello01(name) 方法。
- 第二种，根据方法名获得 Invoker 集合。一般情况下，都能匹配到。
- 第三种，使用全量 Invoker 集合。例如，#$echo(name) 回声方法。
- 第四种，使用 methodInvokerMap 第一个 Invoker 集合。防御性编程。

## 59.4 StaticDirectory ##
com.alibaba.dubbo.rpc.cluster.directory.StaticDirectory ，实现 AbstractDirectory 抽象类，静态 Directory 实现类。逻辑比较简单，将传入的 invokers 集合，封装成静态的 Directory 对象。

ReferenceConfig#createProxy(Map<String, String> map) 使用了此方法。

当 registryURL 非空时，意味着有注册中心，使用 cluster=available 集群方式，并调用 Cluster$Adaptive#join(StaticDirectory) 方法，创建对应的 Cluster Invoker 对象。这意味着，服务调用时，因为使用的是 cluster=available ，仅调用第一个可用的 Invoker 对象。

## 59.5 ClusterUtils ##
com.alibaba.dubbo.rpc.cluster.support.ClusterUtils ，Cluster 工具类。

将 localMap 和 remoteUrl.parameters 合并成 map 。
将合并的 map 的结果，覆盖设置到 remoteUrl 中。

----

# 60 集群容错（四）之 LoadBalance 实现 #
内置了四种负载均衡的选择算法。

## 60.1 LoadBalance ##

com.alibaba.dubbo.rpc.cluster.LoadBalance ， LoadBalance 接口。

- @SPI(RandomLoadBalance.NAME) 注解，Dubbo SPI 拓展点，默认为 "random" ，即随机。
- @Adaptive 注解，基于 Dubbo SPI Adaptive 机制，加载对应的 Cluster 实现，使用 URL.loadbalance 属性。
- selectList<Invoker<T>>, URL, Invocation) 接口方法，从 Invoker 集合中，选择一个。

## 60.2 AbstractLoadBalance ##
com.alibaba.dubbo.rpc.cluster.loadbalance.AbstractLoadBalance ，实现 LoadBalance 接口，LoadBalance 抽象类，提供了权重计算的功能。

### 60.2.1 select ###
select(List<Invoker<T>>, URL, Invocation) 实现方法，默认只有一个 Invoker 时，直接选择返回。

- 子类实现 #doSelect(List<Invoker<T>>, URL, Invocation) 抽象方法，提供自定义的负载均衡策略。

### 60.2.2 getWeight ###
获得 weight 配置

````
protected int getWeight(Invoker<?> invoker, Invocation invocation) {
    // 获得 weight 配置，即服务权重。默认为 100
    int weight = invoker.getUrl().getMethodParameter(invocation.getMethodName(), Constants.WEIGHT_KEY, Constants.DEFAULT_WEIGHT);
    if (weight > 0) {
        long timestamp = invoker.getUrl().getParameter(Constants.REMOTE_TIMESTAMP_KEY, 0L);
        if (timestamp > 0L) {
            // 获得启动总时长
            int uptime = (int) (System.currentTimeMillis() - timestamp);
            // 获得预热需要总时长。默认为 10 * 60 * 1000 = 10 分钟
            int warmup = invoker.getUrl().getParameter(Constants.WARMUP_KEY, Constants.DEFAULT_WARMUP);
            // 处于预热中，计算当前的权重
            if (uptime > 0 && uptime < warmup) {
                weight = calculateWarmupWeight(uptime, warmup, weight);
            }
        }
    }
    return weight;
}
````

考虑到 JVM 自身会有预热的过程，所以服务提供者一启动就直接承担 100% 的流量，可能会出现很吃力的情况。因此权重的计算，默认自带了预热的过程。

calculateWarmupWeight(int uptime, int warmup, int weight) 函数：
计算权重的代码这么写看起来比较“绕”，我们来修改成 (uptime / warmup) * weight ，是否就好理解多了，相当于进度百分比 * 权重。

### 60.3 RandomLoadBalance ###

com.alibaba.dubbo.rpc.cluster.loadbalance.RandomLoadBalance ，实现 AbstractLoadBalance 抽象类，随机，按权重设置随机概率。

特点：在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。

基本流程如下：
1. 计算总权重，并判断所有 Invoker 是否相同权重。
2. 权重不相等，随机权重后，判断在哪个 Invoker 的权重区间中。
3. 权重相等，直接随机选择 Invoker 即可。

### 60.4 RoundRobinLoadBalance ###
com.alibaba.dubbo.rpc.cluster.loadbalance.RoundRobinLoadBalance ，实现 AbstractLoadBalance 抽象类，轮循，按公约后的权重设置轮循比率。

存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。

### 60.5 LeastActiveLoadBalance ###
com.alibaba.dubbo.rpc.cluster.loadbalance.LeastActiveLoadBalance ，实现 AbstractLoadBalance 抽象类，最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。

使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。

相比来说，LeastActiveLoadBalance 是 RandomLoadBalance 的加强版，基于最少活跃调用数。

基本流程如下：
1. 计算获得相同最小活跃数的数组( leastIndexes )和个数( leastCount )。注意，leastIndexes 是重用的，所以需要 leastCount 作为下标。
2. 如果只有一个最小则直接返回。
3. 权重不相等，随机权重后，判断在哪个 Invoker 的权重区间中。
4. 权重相等，直接随机选择 Invoker 即可。

### 60.6 ConsistentHashLoadBalance ###
com.alibaba.dubbo.rpc.cluster.loadbalance.ConsistentHashLoadBalance ，实现 AbstractLoadBalance 抽象类，一致性 Hash，相同参数的请求总是发到同一提供者。

当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。

基本流程如下:
1. 调用 System#identityHashCode(Object) 方法，基于 invokers 集合，根据对象内存地址来计算定义哈希值。
2. 获得 ConsistentHashSelector 对象。若为空，或者定义哈希值变更（说明 invokers 集合发生变化），进行创建新的 ConsistentHashSelector 对象。
3. 调用 ConsistentHashSelector#select(invocation) 方法，选择一个 Invoker 对象。

### 60.6.1 ConsistentHashSelector ###
ConsistentHashSelector ，是 ConsistentHashLoadBalance 的内部类，一致性哈希选择器，基于 Ketama 算法。

#### 60.6.1.1 构造方法 ####

````
/**
 * 虚拟节点与 Invoker 的映射关系
 * 
 */
private final TreeMap<Long, Invoker<T>> virtualInvokers;
/**
 * 每个 Invoker 对应的虚拟节点数，默认为 160 。可通过 <dubbo:parameter key="hash.nodes" value="320" /> 自定义
 */
private final int replicaNumber;
/**
 * 定义哈希值
 */
private final int identityHashCode;
/** 
 * 取值参数位置数组
 * 选择 Invoker 时，计算 Hash 值的参数位置数组，默认为第一个参数。
 * 可通过 <dubbo:parameter key="hash.arguments" value="0,1" /> 自定义
 */
private final int[] argumentIndex;

ConsistentHashSelector(List<Invoker<T>> invokers, String methodName, int identityHashCode) {
    this.virtualInvokers = new TreeMap<Long, Invoker<T>>();
    // 设置 identityHashCode
    this.identityHashCode = identityHashCode;
    URL url = invokers.get(0).getUrl();
    // 初始化 replicaNumber
    this.replicaNumber = url.getMethodParameter(methodName, "hash.nodes", 160);
    // 初始化 argumentIndex
    String[] index = Constants.COMMA_SPLIT_PATTERN.split(url.getMethodParameter(methodName, "hash.arguments", "0"));
    argumentIndex = new int[index.length];
    for (int i = 0; i < index.length; i++) {
        argumentIndex[i] = Integer.parseInt(index[i]);
    }
    // 初始化 virtualInvokers
    for (Invoker<T> invoker : invokers) {
        String address = invoker.getUrl().getAddress();
        // 每四个虚拟结点为一组，为什么这样？下面会说到
        for (int i = 0; i < replicaNumber / 4; i++) {
            // 这组虚拟结点得到惟一名称
            byte[] digest = md5(address + i);
            // Md5是一个16字节长度的数组，将16字节的数组每四个字节一组，分别对应一个虚拟结点，这就是为什么上面把虚拟结点四个划分一组的原因
            for (int h = 0; h < 4; h++) {
                // 对于每四个字节，组成一个long值数值，做为这个虚拟节点的在环中的惟一key
                long m = hash(digest, h);
                virtualInvokers.put(m, invoker);
            }
        }
    }
}
````

基本流程：
1. 循环每个 Invoker 对象。
2. 循环 replicaNumber / 4 次，每四个虚拟节点为一组
3. 拼接 address + i 作为虚拟节点名的唯一名称。调用 #md5(value) 方法，计算 MD5 。
	1. MD5 是一个 16 字节长度的数组，将 16 字节的数组每四个字节一组，分别对应一个虚拟结点，这就是为什么上面把虚拟结点四个划分一组的原因
4. 顺序循环每四个字节。
5. 调用 #hash(byte[] digest, int number) 方法，对于每四个字节，组成一个 Long 值数值，做为这个虚拟节点的在环中的惟一 KEY 。
6. 添加 Invoker 到 virtualInvokers 中。

#### 60.6.1.2 select ####

基本流程如下：
1. 调用 #toKey(Object[] args) 方法，基于方法参数，获得 KEY 。
2. 调用 #md5(key) 方法，计算 MD5 值。
3. 调用 #hash(digest, hash) 方法，计算 KEY 值。
4. 调用 #selectForKey(hash) 方法，选一个 Invoker 对象。代码如下：

----

# 61 集群容错（五）之 Merger 实现 #
分组聚合

## 61.1 Merger ##
com.alibaba.dubbo.rpc.cluster.Merger ，Merger 接口，提供接口方法，将对象数组合并成一个对象。

- @SPI 注解，Dubbo SPI 拓展点，无默认值。

### 61.1.1 Merger 实现类 ###
Merger 内置十二个实现类，从代码上看基本类似

#### 61.1.1.1 MapMerger ####
com.alibaba.dubbo.rpc.cluster.merger.MapMerger ，实现 Merger 接口，Map Merger 实现类

#### 61.1.1.2 ShortArrayMerger ####
com.alibaba.dubbo.rpc.cluster.merger.ShortArrayMerger ，实现 Merger 接口，Short 数组 Merger 实现类。

### 61.1.2 MergerFactory ###
com.alibaba.dubbo.rpc.cluster.merger.MergerFactory ，Merger 工厂类，提供 #getMerger(Class<T> returnType) 方法，获得指定类对应的 Merger 对象。

## 61.2 MergeableCluster ##
com.alibaba.dubbo.rpc.cluster.support.MergeableCluster ，实现 Cluster 接口，分组聚合 Cluster 实现类。
对应 Invoker 实现类为 MergeableClusterInvoker 。

Merger 的使用，需要设置 Cluster 的实现类为 MergeableCluster 。但是呢，它的配置方式，和其他 Cluster 实现类不同。

### 61.2.1 MergeableClusterInvoker ###
com.alibaba.dubbo.rpc.cluster.support.MergeableClusterInvoker ，实现 Invoker 接口，MergeableCluster Invoker 实现类。

基本流程如下：
1. 调用 Directory#list(invocation) 方法，获得服务 Invoker 集合。
2. 调用 URL#getMethodParameter(methodName, "merger") 方法，获得 Merger 拓展名，方法级。
3. 若未配置 Merger 拓展名，优先调用首个可用的 Invoker 对象，其次调用首个 Invoker 对象。
4. 通过反射，获得调用方法的返回类型。
5. 提交线程池，并行执行，发起 RPC 调用，并添加 Future 到 results 中。
6. 阻塞等待执行结果，并添加到 resultList 中。注意，分成正常 Result、异常 Result（忽略）、Exception 三种情况。
	1. 结果大小为空，返回空的 RpcResult 。
	2. 结果大小为 1 ，返回首个 RpcResult 。
	3. 返回类型为 void ，返回空的 RpcResult 。
	4. 
========== 【第 1 种】基于 Method 合并==========
1. 若 merger 为 "." 开头，指定合并方法，将调用返回结果的指定方法进行合并，合并方法的参数类型必须是返回结果类型本身。
2. 调用 Class#getMethod(String name, Class<?>... parameterTypes) 方法，获得合并方法 Method 。这个方法，意味着“合并方法的参数类型必须是返回结果类型本身”！！！，搜索 "在条件分支if ( merger.startsWith(".") ) {}" 。
3. 有 Method ，循环调用 Method#invoke(Object obj, Object... args) 方法，进行合并。
4. 无 Method ，抛出 RpcException 异常。

========== 【第 2 种】基于 Merger 合并 ==========
- 【第 2.1 种】当 merger 为 "default" 或 "true" 时，调用 MergerFactory#getMerger(Class<T> returnType) 方法，根据返回值类型自动匹配 Merger 。
- 【第 2.2 种】调用 ExtensionLoader#getExtension(merger) 方法啊，获得指定 Merger 。
	- 有 Merger ，循环调用 Merger#merge(T... items) 方法，进行合并。
	- 无 Method ，抛出 RpcException 异常。

------

# 62 集群容错（六）之 Configurator 实现 #

实现 Dubbo 的配置规则功能。

## 62.1 ConfiguratorFactory ##
com.alibaba.dubbo.rpc.cluster.ConfiguratorFactory ，Configurator 工厂接口，

- @SPI 注解，Dubbo SPI 拓展点，无默认值。
- @Adaptive("protocol") 注解，基于 Dubbo SPI Adaptive 机制，加载对应的 Configurator 实现，使用 URL.protocol 属性。
- getConfigurator(URL url) 接口方法，获得 Configurator 对象。

### 62.1.1 OverrideConfiguratorFactory ###
com.alibaba.dubbo.rpc.cluster.configurator.override.OverrideConfiguratorFactory ，实现 ConfiguratorFactory 接口，OverrideConfigurator 工厂

### 62.1.2 AbsentConfiguratorFactory ###

com.alibaba.dubbo.rpc.cluster.configurator.absent.AbsentConfiguratorFactory ，实现 ConfiguratorFactory 接口，AbsentConfigurator 工厂。

## 62.2 Configurator ##
com.alibaba.dubbo.rpc.cluster.Configurator ，实现 Comparable 接口，配置规则接口。

- 一个 Configurator 对象，对应一条配置规则。
- Configurator 有优先级的要求，所以实现 Comparable 接口。
- getUrl() 接口方法，获得配置 URL ，里面带有配置规则。
- configure(Url url) 接口方法，设置配置规则到指定 URL 中。

### 62.2.1 AbstractConfigurator ###
com.alibaba.dubbo.rpc.cluster.configurator.AbstractConfigurator ，实现 Configurator 接口，实现公用的配置规则的匹配、排序的逻辑。

#### 62.2.1.1 getUrl ####
获取配置的url

//  todo 剩余补充

# 63 集群容错（七）之 Router 实现 #
实现 Dubbo 的路由规则功能。

//  todo 等待补充

# 64 集群容错（八）之 Mock 实现 #

# 65 优雅停机 #

# 66 日志适配 #
-----